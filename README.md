# MiniTen

**A Lightweight Deep Learning Framework Optimized for Edge Platforms**

[![CI](https://github.com/sumeshthakr/MiniTen/actions/workflows/ci.yml/badge.svg)](https://github.com/sumeshthakr/MiniTen/actions/workflows/ci.yml)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Vision

MiniTen is a high-performance deep learning library designed from the ground up for edge computing. Built purely in Python and Cython with minimal external dependencies, MiniTen delivers exceptional performance in a fraction of the size of traditional frameworks like TensorFlow and PyTorch.

### Why MiniTen?

- **ğŸš€ Optimized for Edge**: Designed specifically for edge platforms (IoT devices, mobile, embedded systems)
- **ğŸ“¦ Minimal Footprint**: Fraction of the size compared to TensorFlow/PyTorch
- **âš¡ High Performance**: Highly optimized Cython implementations for critical operations
- **ğŸ“ Educational**: Clear, well-documented code showing how deep learning works internally
- **ğŸ”§ Modular**: Easy to understand, extend, and contribute to
- **ğŸŒ GPU Support**: Supports CUDA, OpenCL, Metal, and Vulkan for edge GPUs
- **ğŸ”‹ Power Efficient**: Optimized for low-power edge computing scenarios

## âœ¨ Features

### Neural Network Architectures
- **CNNs**: Convolutional Neural Networks for computer vision
- **RNNs**: Recurrent Neural Networks for sequential data
- **LSTMs**: Long Short-Term Memory networks
- **GRUs**: Gated Recurrent Units
- **GNNs**: Graph Neural Networks
- **Transformers**: Attention-based models (coming soon)
- **Reinforcement Learning**: RL algorithms (coming soon)

### Data Processing
- **Vision**: Image processing and augmentation
- **Audio**: Speech and sound processing
- **Video**: Video analysis and processing
- **Text/NLP**: Natural language processing
- **Signal**: Time-series and sensor data processing

### Core Features
- Automatic differentiation
- GPU acceleration (CUDA, OpenCL, Metal, Vulkan)
- Optimized tensor operations in Cython
- Memory-efficient computation
- Model serialization and deployment
- Comprehensive documentation and examples

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/sumeshthakr/MiniTen.git
cd MiniTen

# Install dependencies
pip install -r requirements.txt

# Build and install
python setup.py build_ext --inplace
pip install -e .
```

### Basic Example

```python
import miniten as mt
import numpy as np

# Create a simple neural network (coming soon)
# model = mt.nn.Sequential([
#     mt.nn.Linear(784, 128),
#     mt.nn.ReLU(),
#     mt.nn.Linear(128, 10),
#     mt.nn.Softmax()
# ])

# Current working example with backpropagation
from miniten.core import backprop
bp = backprop.BackPropagation(2, 3, 1)

# Training data (XOR problem)
X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_train = np.array([0, 1, 1, 0])

# Train the network
for epoch in range(1000):
    for X, y in zip(X_train, y_train):
        bp.backward(X, y, learning_rate=0.1)

# Test
for X in X_train:
    output = bp.forward(X)
    print(f"Input: {X}, Output: {output}")
```

## ğŸ“– Documentation

### Project Structure

```
MiniTen/
â”œâ”€â”€ miniten/              # Main package
â”‚   â”œâ”€â”€ core/            # Core tensor operations and autograd
â”‚   â”œâ”€â”€ nn/              # Neural network modules
â”‚   â”‚   â”œâ”€â”€ layers.py    # Common layers (Linear, Conv2d, etc.)
â”‚   â”‚   â”œâ”€â”€ activations.py  # Activation functions
â”‚   â”‚   â”œâ”€â”€ rnn.py       # RNN, LSTM, GRU
â”‚   â”‚   â”œâ”€â”€ cnn.py       # CNN-specific layers
â”‚   â”‚   â””â”€â”€ gnn.py       # Graph neural networks
â”‚   â”œâ”€â”€ optim/           # Optimizers (SGD, Adam, etc.)
â”‚   â”œâ”€â”€ utils/           # Utilities
â”‚   â”‚   â”œâ”€â”€ data.py      # Data loading
â”‚   â”‚   â”œâ”€â”€ vision.py    # Image processing
â”‚   â”‚   â”œâ”€â”€ audio.py     # Audio processing
â”‚   â”‚   â”œâ”€â”€ text.py      # Text/NLP utilities
â”‚   â”‚   â””â”€â”€ signal.py    # Signal processing
â”‚   â””â”€â”€ gpu/             # GPU backends
â”œâ”€â”€ docs/                # Documentation
â”œâ”€â”€ examples/            # Example scripts
â”œâ”€â”€ tests/               # Test suite
â””â”€â”€ benchmarks/          # Performance benchmarks
```

### Key Modules

#### Core (`miniten.core`)
- **Tensor**: Multi-dimensional arrays with autograd
- **Autograd**: Automatic differentiation engine
- **Operations**: Optimized mathematical operations (Cython)

#### Neural Networks (`miniten.nn`)
- **Layers**: Linear, Conv2d, MaxPool2d, Dropout, BatchNorm
- **Activations**: ReLU, Sigmoid, Tanh, Softmax, GELU
- **RNN**: RNN, LSTM, GRU with bidirectional support
- **CNN**: Depthwise separable convolutions, dilated convolutions
- **GNN**: GraphConv, GraphAttention, SAGEConv

#### Optimizers (`miniten.optim`)
- SGD with momentum
- Adam, AdamW, Adamax
- RMSprop, Adagrad
- Learning rate schedulers

#### GPU Support (`miniten.gpu`)
- CUDA for NVIDIA GPUs (including Jetson)
- OpenCL for cross-platform support
- Metal for Apple Silicon
- Vulkan for cross-platform compute

## ğŸ¯ Roadmap

### Phase 1: Foundation (Completed âœ…)
- [x] Project structure and architecture
- [x] Core module stubs (Tensor, Autograd)
- [x] Basic backpropagation (working)
- [x] Vector operations (working and **optimized**)
  - **3.75x faster than NumPy** for large vector operations (100K+ elements)
  - OpenMP parallelization for operations with 10K+ elements
  - Memory views for zero-copy operations
  - Loop unrolling for dot products
- [x] Matrix operations (matmul, transpose)
- [x] Activation functions (ReLU, Sigmoid, Tanh, Softmax) in Cython
- [ ] Complete Tensor implementation with autograd
- [ ] Automatic differentiation engine
- [ ] GPU backend infrastructure

### Phase 2: Neural Networks (In Progress âš¡)
- [x] **Linear layer** (fully connected) with optimized forward/backward
  - He initialization for weights
  - Efficient gradient computation
  - Parameter update support
- [x] **Activation functions** with forward/backward
  - ReLU with mask caching
  - Sigmoid with output caching
  - Softmax with numerical stability
  - Tanh activation
- [x] **Loss functions**
  - Mean Squared Error (MSE)
  - Cross Entropy with numerical stability
- [x] **Training verified**: Two-layer network achieves 98.8% loss reduction
- [ ] Conv2d layer with optimized convolution kernels
- [ ] Pooling layers (MaxPool2d, AvgPool2d)
- [ ] Model containers (Sequential, ModuleList)
- [ ] RNN/LSTM/GRU implementations
- [ ] CNN optimizations for edge

### Phase 3: Advanced Features
- [ ] Graph Neural Networks
- [ ] Attention mechanisms
- [ ] Transformer architecture
- [ ] Reinforcement Learning basics
- [ ] Model quantization
- [ ] Pruning and compression

### Phase 4: Optimization & Deployment
- [x] OpenMP parallelization (implemented)
- [x] Memory-efficient Cython operations
- [ ] GPU kernel optimization
- [ ] SIMD optimizations
- [ ] Memory pooling
- [ ] Model serialization
- [ ] Edge deployment tools
- [ ] Comprehensive benchmarking suite

### Phase 5: Data Processing
- [ ] Image processing pipeline
- [ ] Audio processing pipeline
- [ ] Video processing
- [ ] NLP utilities
- [ ] Signal processing
- [ ] Data augmentation

## ğŸ¤ Contributing

We welcome contributions! MiniTen is designed to be an educational and collaborative project.

### How to Contribute

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Make your changes**
4. **Add tests** for your changes
5. **Ensure tests pass**: `python -m pytest tests/`
6. **Commit your changes**: `git commit -m 'Add amazing feature'`
7. **Push to branch**: `git push origin feature/amazing-feature`
8. **Open a Pull Request**

### Development Guidelines

- Write clear, documented code
- Follow existing code style
- Add comprehensive tests
- Update documentation
- Optimize for edge devices
- Minimize external dependencies

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## ğŸ“Š Performance

### Benchmark Results

MiniTen provides highly optimized Cython implementations with **OpenMP parallelization** that can **outperform NumPy** for large-scale operations.

**ğŸš€ Optimized Vector Operations (vs NumPy):**

| Operation | Small (100) | Medium (10K) | Large (100K) | **Performance** |
|-----------|-------------|--------------|--------------|-----------------|
| Vector Addition | 0.29x | 1.05x | **3.75x faster** âš¡ | Parallel speedup |
| Element-wise Multiply | 0.27x | 1.01x | **3.43x faster** âš¡ | Parallel speedup |
| Dot Product | 0.73x | 0.31x | 0.11x | NumPy BLASå„ªã‚Œ |

**Key Findings:**
- âœ… **3.75x faster than NumPy** for large vector operations (100K+ elements)
- âœ… OpenMP parallelization provides significant speedup for large datasets
- âœ… Memory views eliminate copy overhead
- âœ… Loop unrolling improves dot product performance
- âš ï¸ NumPy's BLAS/LAPACK still faster for small operations due to overhead
- âš ï¸ Matrix multiplication needs further optimization (tiled algorithms planned)

**Neural Network Training/Inference (vs Pure Python):**

| Configuration | Training Speedup | Inference Speedup |
|--------------|-----------------|------------------|
| XOR (2-4-1)  | 1.44x | 1.86x |
| XOR (2-16-1) | 1.30x | 1.88x |
| XOR (2-64-1) | 1.29x | 1.86x |

**Neural Network Layers (Cython-optimized):**

| Layer | Implementation Status | Optimization |
|-------|----------------------|--------------|
| Linear | âœ… Complete | Optimized matmul, He init |
| ReLU | âœ… Complete | Mask caching, 1.11x vs NumPy |
| Sigmoid | âœ… Complete | Output caching |
| Softmax | âœ… Complete | Numerical stability |
| MSE Loss | âœ… Complete | Gradient computation |
| Cross Entropy | âœ… Complete | Numerical stability |

**Training Performance:**
- Two-layer network (2â†’4â†’2): **98.8% loss reduction** in 100 epochs
- Full forward/backward pass implementation
- Gradient computation and parameter updates working

**Old Vector Operations (Legacy, vs NumPy):**

| Operation | Small Vectors (100) | Large Vectors (100K) |
|-----------|---------------------|----------------------|
| Vector Addition | 1.95x slower | 1.22x slower |
| Dot Product | 1.14x faster | 38x slower |
| Element-wise Multiply | 1.88x slower | 1.01x slower |
| Scalar Multiply | 1.17x faster | 1.76x slower |

> **Note**: NumPy uses highly optimized BLAS/LAPACK libraries with SIMD instructions.
> MiniTen's focus is on educational value, minimal footprint, and edge computingâ€”not
> competing with production frameworks on raw speed.

### MiniTen's Advantages

- **Minimal footprint**: Pure Python/Cython with minimal dependencies
- **Educational**: Clear, readable code for learning ML internals
- **Customizable**: Easy to extend for custom hardware or specialized use cases
- **Edge-optimized**: Designed for resource-constrained environments

See detailed benchmarks in the [`benchmarks/`](benchmarks/) directory.

## ğŸ”§ Requirements

- Python 3.8+
- Cython 0.29+
- NumPy (minimal usage, only where necessary)

Optional:
- CUDA toolkit (for NVIDIA GPU support)
- OpenCL (for cross-platform GPU support)
- Metal (automatically available on macOS/iOS)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

MiniTen is inspired by:
- PyTorch's design philosophy
- TensorFlow Lite's edge optimization
- Tinygrad's minimalism
- Educational resources from Fast.ai and Andrej Karpathy

## ğŸ“¬ Contact

- **GitHub Issues**: [Report bugs or request features](https://github.com/sumeshthakr/MiniTen/issues)
- **Discussions**: [Join the discussion](https://github.com/sumeshthakr/MiniTen/discussions)

## ğŸŒŸ Show Your Support

If you find MiniTen useful, please consider:
- â­ Starring the repository
- ğŸ› Reporting bugs
- ğŸ’¡ Suggesting new features
- ğŸ“ Contributing code or documentation
- ğŸ“¢ Spreading the word

---

**Built with â¤ï¸ for the edge computing community**
